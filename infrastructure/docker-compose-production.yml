# Production Docker Compose Configuration for Short Video Maker
# Supports horizontal scaling, high availability, and monitoring
version: '3.8'

services:
  # Load Balancer & Reverse Proxy
  haproxy:
    image: haproxy:2.8-alpine
    ports:
      - "80:80"
      - "443:443"
      - "8404:8404"  # HAProxy Stats
    volumes:
      - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./certs:/etc/ssl/certs:ro
    depends_on:
      - api-gateway
    networks:
      - frontend
      - backend
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8404/stats"]
      interval: 30s
      timeout: 10s
      retries: 3

  # API Gateway Cluster
  api-gateway:
    build:
      context: .
      dockerfile: main.Dockerfile
      target: prod-deps
    environment:
      - NODE_ENV=production
      - PORT=3123
      - REDIS_URL=redis://redis-cluster:6379
      - DATABASE_URL=postgresql://user:password@postgres-primary:5432/video_db
      - GPU_POOL_ENABLED=true
      - MONITORING_ENABLED=true
      - LOG_LEVEL=info
    ports:
      - "3123"
    volumes:
      - ./data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - redis-cluster
      - postgres-primary
    networks:
      - backend
      - gpu-network
    deploy:
      replicas: 5
      placement:
        constraints:
          - node.labels.tier == api
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3123/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # GPU Worker Pools
  gpu-worker-cuda:
    build:
      context: ..
      dockerfile: main-cuda.Dockerfile
    environment:
      - NODE_ENV=production
      - WORKER_TYPE=gpu-cuda
      - GPU_MEMORY_FRACTION=0.8
      - CUDA_VISIBLE_DEVICES=0
      - REDIS_URL=redis://redis-cluster:6379
      - CONCURRENT_JOBS=4
      - QUEUE_NAME=gpu-processing
      - FRAMEPACK_PATH=/app/FramePack
      - FRAMEPACK_VENV=/app/FramePack/venv/bin/python
    volumes:
      - ./data:/app/data
      - gpu-cache:/app/cache
    depends_on:
      - redis-cluster
      - prometheus
    networks:
      - gpu-network
      - backend
    deploy:
      replicas: 3
      placement:
        constraints:
          - node.labels.gpu == nvidia
          - node.labels.gpu.memory >= 8GB
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 2
      resources:
        reservations:
          generic_resources:
            - discrete_resource_spec:
                kind: 'gpu'
                value: 1
        limits:
          cpus: '6.0'
          memory: 16G
    healthcheck:
      test: ["CMD", "nvidia-smi"]
      interval: 60s
      timeout: 30s
      retries: 2

  gpu-worker-cpu:
    build:
      context: .
      dockerfile: main.Dockerfile
    environment:
      - NODE_ENV=production
      - WORKER_TYPE=gpu-cpu
      - REDIS_URL=redis://redis-cluster:6379
      - CONCURRENT_JOBS=2
      - QUEUE_NAME=cpu-processing
      - FFMPEG_THREADS=4
    volumes:
      - ./data:/app/data
      - cpu-cache:/app/cache
    depends_on:
      - redis-cluster
    networks:
      - backend
    deploy:
      replicas: 8
      placement:
        constraints:
          - node.labels.tier == worker
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 3
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3123/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Cluster for Queue & Cache
  redis-cluster:
    image: redis/redis-stack-server:7.2.0-v8
    ports:
      - "6379"
    volumes:
      - redis-data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - backend
    deploy:
      replicas: 3
      placement:
        constraints:
          - node.labels.tier == cache
      restart_policy:
        condition: on-failure
        delay: 10s
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    sysctls:
      - net.core.somaxconn=65535

  redis-sentinel:
    image: redis:7.2-alpine
    command: redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./redis/sentinel.conf:/etc/redis/sentinel.conf:ro
    depends_on:
      - redis-cluster
    networks:
      - backend
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure

  # PostgreSQL Primary-Replica Setup
  postgres-primary:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: video_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: replicator_password
    volumes:
      - postgres-primary-data:/var/lib/postgresql/data
      - ./postgres/primary.conf:/etc/postgresql/postgresql.conf:ro
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "5432"
    networks:
      - backend
    deploy:
      placement:
        constraints:
          - node.labels.postgres == primary
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d video_db"]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres-replica:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      PGUSER: user
    volumes:
      - postgres-replica-data:/var/lib/postgresql/data
      - ./postgres/replica.conf:/etc/postgresql/postgresql.conf:ro
    depends_on:
      - postgres-primary
    networks:
      - backend
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.postgres == replica
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring Stack
  prometheus:
    image: prom/prometheus:v2.47.0
    ports:
      - "9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - backend
      - monitoring
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  grafana:
    image: grafana/grafana:10.1.0
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    ports:
      - "3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    networks:
      - monitoring
      - frontend
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  alertmanager:
    image: prom/alertmanager:v0.26.0
    ports:
      - "9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    depends_on:
      - prometheus
    networks:
      - monitoring
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Log Aggregation
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100"
    volumes:
      - ./monitoring/loki/loki.yml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    networks:
      - monitoring
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3

  promtail:
    image: grafana/promtail:2.9.0
    volumes:
      - ./monitoring/promtail/promtail.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    depends_on:
      - loki
    networks:
      - monitoring
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
    command: -config.file=/etc/promtail/config.yml

  # Node Exporter for System Metrics
  node-exporter:
    image: prom/node-exporter:v1.6.1
    ports:
      - "9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - monitoring
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

  # GPU Metrics Exporter
  nvidia-gpu-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.1.8-3.1.5-ubuntu20.04
    ports:
      - "9400"
    volumes:
      - /var/lib/kubelet/pod-resources:/var/lib/kubelet/pod-resources:ro
    networks:
      - monitoring
    deploy:
      placement:
        constraints:
          - node.labels.gpu == nvidia
      restart_policy:
        condition: on-failure
    privileged: true

  # MinIO for Object Storage
  minio:
    image: minio/minio:RELEASE.2023-09-04T19-57-37Z
    ports:
      - "9000"
      - "9001"  # Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
      MINIO_PROMETHEUS_AUTH_TYPE: public
    volumes:
      - minio-data:/data
    networks:
      - backend
    deploy:
      replicas: 4
      placement:
        constraints:
          - node.labels.storage == minio
      restart_policy:
        condition: on-failure
    command: server http://minio:9000/data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Jaeger for Distributed Tracing
  jaeger:
    image: jaegertracing/all-in-one:1.49
    ports:
      - "16686"  # UI
      - "14268"  # Collector HTTP
      - "14250"  # Collector gRPC
    environment:
      COLLECTOR_OTLP_ENABLED: true
    volumes:
      - jaeger-data:/badger
    networks:
      - monitoring
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Auto-scaler Service
  autoscaler:
    build:
      context: ./autoscaler
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - DOCKER_HOST=unix:///var/run/docker.sock
      - PROMETHEUS_URL=http://prometheus:9090
      - SCALING_ENABLED=true
      - MIN_REPLICAS=2
      - MAX_REPLICAS=20
      - SCALE_UP_THRESHOLD=80
      - SCALE_DOWN_THRESHOLD=20
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - prometheus
    networks:
      - backend
      - monitoring
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure

networks:
  frontend:
    driver: overlay
    attachable: true
    driver_opts:
      encrypted: "true"
  backend:
    driver: overlay
    internal: true
    driver_opts:
      encrypted: "true"
  gpu-network:
    driver: overlay
    internal: true
    driver_opts:
      encrypted: "true"
  monitoring:
    driver: overlay
    internal: true
    driver_opts:
      encrypted: "true"

volumes:
  postgres-primary-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/postgres-primary
      o: bind
  postgres-replica-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/postgres-replica
      o: bind
  redis-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/redis
      o: bind
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/prometheus
      o: bind
  grafana-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/grafana
      o: bind
  alertmanager-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/alertmanager
      o: bind
  loki-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/loki
      o: bind
  minio-data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/minio
      o: bind
  jaeger-data:
    driver: local
  gpu-cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=2g,noexec,nosuid,nodev
  cpu-cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=1g,noexec,nosuid,nodev

configs:
  haproxy_config:
    file: ./haproxy/haproxy.cfg
  prometheus_config:
    file: ./monitoring/prometheus/prometheus.yml
  grafana_datasources:
    file: ./monitoring/grafana/datasources/prometheus.yml

secrets:
  postgres_password:
    external: true
  redis_password:
    external: true
  minio_credentials:
    external: true